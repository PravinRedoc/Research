{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# please don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "# make_classification is used to create custom dataset \n",
    "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#please don't change random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0001, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='constant', loss='log', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=15, shuffle=True, tol=0.001,\n",
       "       validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.76, NNZs: 15, Bias: -0.314605, T: 37500, Avg. loss: 0.455801\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.92, NNZs: 15, Bias: -0.469578, T: 75000, Avg. loss: 0.394737\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.98, NNZs: 15, Bias: -0.580452, T: 112500, Avg. loss: 0.385561\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.02, NNZs: 15, Bias: -0.660824, T: 150000, Avg. loss: 0.382161\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.717218, T: 187500, Avg. loss: 0.380474\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.761816, T: 225000, Avg. loss: 0.379481\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.793932, T: 262500, Avg. loss: 0.379096\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.07, NNZs: 15, Bias: -0.820446, T: 300000, Avg. loss: 0.378826\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.07, NNZs: 15, Bias: -0.840093, T: 337500, Avg. loss: 0.378604\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.08, NNZs: 15, Bias: -0.850329, T: 375000, Avg. loss: 0.378615\n",
      "Total training time: 0.05 seconds.\n",
      "Convergence after 10 epochs took 0.05 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0001, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='constant', loss='log', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=15, shuffle=True, tol=0.001,\n",
       "       validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.42328902,  0.18380407, -0.14437354,  0.34064016, -0.21316099,\n",
       "          0.56702655, -0.44910569, -0.09094413,  0.21219292,  0.17750247,\n",
       "          0.19931732, -0.00506998, -0.07781235,  0.33343476,  0.0320374 ]]),\n",
       " (1, 15),\n",
       " array([-0.85032916]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
    "\n",
    "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
    "\n",
    "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
    "\n",
    "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
    "\n",
    "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
    "\n",
    "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
    "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
    "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
    "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
    "        you can stop the training\n",
    "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights to zeros array of (dim,1) dimensions\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    w= np.zeros_like(dim)\n",
    "    b=0\n",
    "\n",
    "    return w,b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7I6uWBRsKc4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv1llH429wG5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "def grader_weights(w,b):\n",
    "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "  return True\n",
    "grader_weights(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sig = 1/(1+np.exp(-z))\n",
    "\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_JASp_NAfK_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "  val=sigmoid(z)\n",
    "  assert(val==0.8807970779778823)\n",
    "  return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    loss=0\n",
    "    for i in range(len(y_true)):\n",
    "        loss+=((y_true[i]*np.log10(y_pred[i]))+((1-y_true[i])*np.log10((1-y_pred[i]))))\n",
    "        \n",
    "    loss =(-1*loss)/len(y_true)\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzttjvBFCuQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_logloss(true,pred):\n",
    "  loss=logloss(true,pred)\n",
    "  assert(loss==0.07644900402910389)\n",
    "  return True\n",
    "true=[1,1,0,1,0]\n",
    "pred=[0.9,0.8,0.1,0.8,0.2]\n",
    "grader_logloss(true,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    dw0 = np.dot(w, x) + b\n",
    "    dw = x*(y - sigmoid(dw0)) - ((alpha/(N)) * w)\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI3xD8ctGEnJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "  assert(np.sum(grad_dw)==2.613689585)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LE8g84_GI62n"
   },
   "source": [
    "### <font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "#### $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    " def gradient_db(x,y,w,b):\n",
    "    '''In this function, we will compute gradient w.r.to b '''\n",
    "    db0 = np.dot(w, x) + b\n",
    "    db = y - sigmoid(db0)\n",
    "    return db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "  grad_db=gradient_db(x,y,w,b)\n",
    "  assert(grad_db==-0.5)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_proba(w,b, X):\n",
    "    #print(\"Xtra\",X)\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b       \n",
    "        predict.append(sigmoid(z))\n",
    "    return np.array(predict)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    #implement the code as follows\n",
    "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    w,b = initialize_weights(X_train[0]) \n",
    "    # for every epoch\n",
    "    gw,gb = 0,0\n",
    "    trainloss_list = []\n",
    "    testloss_list = []\n",
    "    N = len(X_train)\n",
    "    for i in range(epochs):\n",
    "        print(\"Epoch: \"+str(i))\n",
    "        # for every data point(X_train,y_train)\n",
    "        for i in range(N):\n",
    "            gw=gradient_dw(X_train[i],y_train[i],w,b,alpha,N)\n",
    "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
    "            gb=gradient_db(X_train[i],y_train[i],w,b)\n",
    "           #compute gradient w.r.to b (call the gradient_db() function)\n",
    "           #update w, b\n",
    "            w=w+(eta0*gw)\n",
    "            b=b+(eta0*gb)\n",
    "        # predict the output of x_train[for all data points in X_train] using w,b\n",
    "        #print(w,b)\n",
    "        ytrain_pred = find_proba(w,b, X_train)\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        losstr =logloss(y_train,ytrain_pred)\n",
    "        print(\"Train loss: \",losstr)\n",
    "        # store all the train loss values in a list\n",
    "        trainloss_list.append(losstr)\n",
    "        # predict the output of x_test[for all data points in X_test] using w,b\n",
    "        ytest_pred = find_proba(w,b, X_test)\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        losste =logloss(y_test,ytest_pred)\n",
    "        # store all the test loss values in a list\n",
    "        print(\"Test loss: \",losste)\n",
    "        testloss_list.append(losste)\n",
    "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
    "        \n",
    "    return w,b,trainloss_list,testloss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUquz7LFEZ6E",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train loss:  0.1754574844285461\n",
      "Test loss:  0.1759547442321374\n",
      "Epoch: 1\n",
      "Train loss:  0.16867157050333045\n",
      "Test loss:  0.16939931358951013\n",
      "Epoch: 2\n",
      "Train loss:  0.1663916799246292\n",
      "Test loss:  0.16720591194885742\n",
      "Epoch: 3\n",
      "Train loss:  0.1653682753740316\n",
      "Test loss:  0.16621717799334954\n",
      "Epoch: 4\n",
      "Train loss:  0.16485707459547086\n",
      "Test loss:  0.16571959463978406\n",
      "Epoch: 5\n",
      "Train loss:  0.1645882001292827\n",
      "Test loss:  0.16545557095508645\n",
      "Epoch: 6\n",
      "Train loss:  0.16444271323364382\n",
      "Test loss:  0.1653113502079951\n",
      "Epoch: 7\n",
      "Train loss:  0.16436263615826985\n",
      "Test loss:  0.16523116853179268\n",
      "Epoch: 8\n",
      "Train loss:  0.16431806946667749\n",
      "Test loss:  0.1651860589844903\n",
      "Epoch: 9\n",
      "Train loss:  0.1642930737413251\n",
      "Test loss:  0.16516045651849878\n",
      "Epoch: 10\n",
      "Train loss:  0.16427897430934066\n",
      "Test loss:  0.16514582028704108\n",
      "Epoch: 11\n",
      "Train loss:  0.16427098545835506\n",
      "Test loss:  0.1651373983536637\n",
      "Epoch: 12\n",
      "Train loss:  0.1642664419100352\n",
      "Test loss:  0.16513252084404828\n",
      "Epoch: 13\n",
      "Train loss:  0.16426384911424854\n",
      "Test loss:  0.1651296765934633\n",
      "Epoch: 14\n",
      "Train loss:  0.16426236468266478\n",
      "Test loss:  0.1651280051869749\n",
      "Epoch: 15\n",
      "Train loss:  0.16426151190514926\n",
      "Test loss:  0.16512701421034165\n",
      "Epoch: 16\n",
      "Train loss:  0.16426102013167446\n",
      "Test loss:  0.16512642050180534\n",
      "Epoch: 17\n",
      "Train loss:  0.16426073527505572\n",
      "Test loss:  0.16512606043934\n",
      "Epoch: 18\n",
      "Train loss:  0.1642605693884229\n",
      "Test loss:  0.16512583897905464\n",
      "Epoch: 19\n",
      "Train loss:  0.16426047215122602\n",
      "Test loss:  0.1651257005820187\n",
      "Epoch: 20\n",
      "Train loss:  0.16426041469677802\n",
      "Test loss:  0.16512561256556316\n",
      "Epoch: 21\n",
      "Train loss:  0.16426038041728624\n",
      "Test loss:  0.1651255555353611\n",
      "Epoch: 22\n",
      "Train loss:  0.16426035972510464\n",
      "Test loss:  0.1651255178667426\n",
      "Epoch: 23\n",
      "Train loss:  0.1642603470622927\n",
      "Test loss:  0.16512549250888583\n",
      "Epoch: 24\n",
      "Train loss:  0.1642603391903866\n",
      "Test loss:  0.1651254751256673\n",
      "Epoch: 25\n",
      "Train loss:  0.1642603342104248\n",
      "Test loss:  0.16512546300819161\n",
      "Epoch: 26\n",
      "Train loss:  0.1642603310001301\n",
      "Test loss:  0.16512545443448257\n",
      "Epoch: 27\n",
      "Train loss:  0.16426032888986591\n",
      "Test loss:  0.16512544828939998\n",
      "Epoch: 28\n",
      "Train loss:  0.16426032747541439\n",
      "Test loss:  0.16512544383683092\n",
      "Epoch: 29\n",
      "Train loss:  0.16426032650945105\n",
      "Test loss:  0.16512544058152992\n",
      "Epoch: 30\n",
      "Train loss:  0.16426032583825453\n",
      "Test loss:  0.16512543818419045\n",
      "Epoch: 31\n",
      "Train loss:  0.16426032536459445\n",
      "Test loss:  0.16512543640841268\n",
      "Epoch: 32\n",
      "Train loss:  0.16426032502581875\n",
      "Test loss:  0.16512543508700558\n",
      "Epoch: 33\n",
      "Train loss:  0.16426032478075375\n",
      "Test loss:  0.16512543410018002\n",
      "Epoch: 34\n",
      "Train loss:  0.16426032460180728\n",
      "Test loss:  0.16512543336117225\n",
      "Epoch: 35\n",
      "Train loss:  0.16426032447014866\n",
      "Test loss:  0.16512543280655914\n",
      "Epoch: 36\n",
      "Train loss:  0.1642603243726961\n",
      "Test loss:  0.16512543238964958\n",
      "Epoch: 37\n",
      "Train loss:  0.16426032430021448\n",
      "Test loss:  0.16512543207585426\n",
      "Epoch: 38\n",
      "Train loss:  0.164260324246105\n",
      "Test loss:  0.16512543183944395\n",
      "Epoch: 39\n",
      "Train loss:  0.16426032420559733\n",
      "Test loss:  0.1651254316612056\n",
      "Epoch: 40\n",
      "Train loss:  0.16426032417520106\n",
      "Test loss:  0.165125431526748\n",
      "Epoch: 41\n",
      "Train loss:  0.16426032415235475\n",
      "Test loss:  0.16512543142527641\n",
      "Epoch: 42\n",
      "Train loss:  0.16426032413516098\n",
      "Test loss:  0.16512543134867216\n",
      "Epoch: 43\n",
      "Train loss:  0.16426032412221028\n",
      "Test loss:  0.1651254312908281\n",
      "Epoch: 44\n",
      "Train loss:  0.16426032411244415\n",
      "Test loss:  0.16512543124714218\n",
      "Epoch: 45\n",
      "Train loss:  0.16426032410507815\n",
      "Test loss:  0.16512543121414353\n",
      "Epoch: 46\n",
      "Train loss:  0.1642603240995188\n",
      "Test loss:  0.16512543118921494\n",
      "Epoch: 47\n",
      "Train loss:  0.16426032409532287\n",
      "Test loss:  0.1651254311703816\n",
      "Epoch: 48\n",
      "Train loss:  0.16426032409215466\n",
      "Test loss:  0.16512543115615208\n",
      "Epoch: 49\n",
      "Train loss:  0.16426032408976063\n",
      "Test loss:  0.1651254311454016\n"
     ]
    }
   ],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=50\n",
    "w,b,trainloss_list,testloss_list=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "<font color='red'>Goal of assignment</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx8Rs9rfEZ1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00650342  0.00923113 -0.0040964  -0.00254652 -0.00812134  0.00292237\n",
      "   0.00392405  0.00104009  0.00963658 -0.00367283 -0.00056886  0.00448568\n",
      "  -0.00352176  0.00565534 -0.00904944]] [-0.04192301]\n",
      "[[-0.42328902  0.18380407 -0.14437354  0.34064016 -0.21316099  0.56702655\n",
      "  -0.44910569 -0.09094413  0.21219292  0.17750247  0.19931732 -0.00506998\n",
      "  -0.07781235  0.33343476  0.0320374 ]] [-0.85032916]\n",
      "[-0.42979243  0.1930352  -0.14846994  0.33809364 -0.22128233  0.56994892\n",
      " -0.44518163 -0.08990403  0.2218295   0.17382964  0.19874846 -0.0005843\n",
      " -0.08133411  0.3390901   0.02298796] -0.8922521633477\n"
     ]
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "print(w-clf.coef_, b-clf.intercept_)\n",
    "\n",
    "print(clf.coef_,clf.intercept_)\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
    "\n",
    "* epoch number on X-axis\n",
    "* loss on Y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1O6GrRt7UeCJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(epochs),trainloss_list)\n",
    "plt.plot(range(epochs),testloss_list)\n",
    "\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train','Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUN8puFoEZtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9522133333333334\n",
      "0.95\n"
     ]
    }
   ],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
    "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-k28U1xDsLIO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMokBfs3-2PY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
